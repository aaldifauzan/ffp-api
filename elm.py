# -*- coding: utf-8 -*-
"""elm

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X_PMYt5BHdrIITdHwMb8CyuWGlMg7Siz
"""

from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.linear_model import Ridge
import numpy as np
import pinv

# class elm():
    # def _init_(self, hidden_units, activation_function, x, y, random_type):
    #     self.hidden_units = hidden_units
    #     self.activation_function = activation_function
    #     self.x = x
    #     self.y = y
    #     self.beta = np.zeros((self.hidden_units, 1))
    #     self.random_type = random_type

    #     # Randomly generate the weight matrix and bias vector from input to hidden layer
    #     if self.random_type == 'uniform':
    #         self.W = np.random.uniform(low=0, high=1, size=(self.hidden_units, self.x.shape[1]))
    #         self.b = np.random.uniform(low=0, high=1, size=(self.hidden_units, 1))
    #     if self.random_type == 'normal':
    #         self.W = np.random.normal(loc=0, scale=0.5, size=(self.hidden_units, self.x.shape[1]))
    #         self.b = np.random.normal(loc=0, scale=0.5, size=(self.hidden_units, 1))

    # def __input2hidden(self, x):
    #     '''
    #     Compute the output of hidden layer using different activation functions.
    #     '''
    #     temH = np.dot(self.W, x.T) + self.b
    #     if self.activation_function == 'sigmoid':
    #         return 1 / (1 + np.exp(-temH))
    #     elif self.activation_function == 'relu':
    #         return np.maximum(0, temH)
    #     elif self.activation_function == 'tanh':
    #         return np.tanh(temH)
    #     elif self.activation_function == 'leaky_relu':
    #         return np.maximum(0, temH) + 0.01 * np.minimum(0, temH)
    #     elif self.activation_function == 'linear':
    #         return temH
    #     elif self.activation_function == 'softplus':
    #         return np.log1p(np.exp(temH))

    # def fit(self):
    #     '''
    #     Train the model using a regularization technique.
    #     '''
    #     H = self.__input2hidden(self.x)
    #     # Regularization with identity matrix scaled by the inverse of C
    #     self.beta = np.dot(pinv(np.dot(H, H.T)), np.dot(H, self.y))

    # def predict(self, x):
    #     H = self.__input2hidden(x)
    #     return np.dot(H.T, self.beta)

    # def score(self, x_test, y_test):
    #       y_pred = self.predict(x_test)
    #       mae = np.mean(np.abs(y_pred - y_test))
    #       mse = np.mean((y_pred - y_test) ** 2)
    #       r_squared = 1 - (np.sum((y_test - y_pred) * 2) / np.sum((y_test - np.mean(y_test)) * 2))
    #       return mae,mse,r_squared
    
class ExtremeLearningMachine(BaseEstimator, RegressorMixin):
    def __init__(self, n_hidden_units=30, n_hidden_layers=7, dropout_prob=0.5, C=10):
        self.n_hidden_units = n_hidden_units
        self.n_hidden_layers = n_hidden_layers
        self.dropout_prob = dropout_prob
        self.C = C
        self.ridge_reg = Ridge(alpha=C)

    def fit(self, X, y):
        X, y = check_X_y(X, y)
        # Implement your ELM fitting here
        # Example: Generate random weights and biases for the hidden layer
        self.hidden_weights = np.random.randn(X.shape[1], self.n_hidden_units)
        self.hidden_biases = np.random.randn(self.n_hidden_units)
        # Calculate hidden layer output
        hidden_output = np.maximum(0, np.dot(X, self.hidden_weights) + self.hidden_biases)
        # Fit ridge regression on hidden layer output
        self.ridge_reg.fit(hidden_output, y)
        self.is_fitted_ = True
        return self

    def predict(self, X):
        check_is_fitted(self)
        X = check_array(X)
        hidden_output = np.maximum(0, np.dot(X, self.hidden_weights) + self.hidden_biases)
        return self.ridge_reg.predict(hidden_output)